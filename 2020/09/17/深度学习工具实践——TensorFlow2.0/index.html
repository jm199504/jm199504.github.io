<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>深度学习工具实践——TensorFlow2.0 | Jimmy&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="概述：深度学习工具实践——TensorFlow2.0">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习工具实践——TensorFlow2.0">
<meta property="og:url" content="http://example.com/2020/09/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94TensorFlow2.0/index.html">
<meta property="og:site_name" content="Jimmy&#39;s blog">
<meta property="og:description" content="概述：深度学习工具实践——TensorFlow2.0">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/tf2/1.png">
<meta property="og:image" content="http://example.com/images/tf2/2.png">
<meta property="og:image" content="http://example.com/images/tf2/3.png">
<meta property="og:image" content="http://example.com/images/tf2/4.png">
<meta property="og:image" content="http://example.com/images/tf2/5.png">
<meta property="og:image" content="http://example.com/images/tf2/6.png">
<meta property="og:image" content="http://example.com/images/tf2/7.png">
<meta property="og:image" content="http://example.com/images/tf2/8.png">
<meta property="og:image" content="http://example.com/images/tf2/9.png">
<meta property="article:published_time" content="2020-09-17T15:21:12.799Z">
<meta property="article:modified_time" content="2021-04-01T05:20:00.203Z">
<meta property="article:author" content="Jimmy Guo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/tf2/1.png">
  
    <link rel="alternate" href="/atom.xml" title="Jimmy's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Jimmy&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-深度学习工具实践——TensorFlow2.0" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/09/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94TensorFlow2.0/" class="article-date">
  <time class="dt-published" datetime="2020-09-17T15:21:12.799Z" itemprop="datePublished">2020-09-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tools/">Tools</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      深度学习工具实践——TensorFlow2.0
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>概述：深度学习工具实践——TensorFlow2.0</p>
<span id="more"></span>

<h1 id="1-基本语法"><a href="#1-基本语法" class="headerlink" title="1.基本语法"></a>1.基本语法</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br><span class="line"><span class="comment"># 无关信息是CPP写出的，0全部打印，2只打印error信息</span></span><br><span class="line">import os</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="2-构建单层感知机"><a href="#2-构建单层感知机" class="headerlink" title="2.构建单层感知机"></a>2.构建单层感知机</h1><p>预测f(x)=ax+b</p>
<h2 id="2-1导入库"><a href="#2-1导入库" class="headerlink" title="2.1导入库"></a>2.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import random</span><br><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure>

<h2 id="2-2生成数据"><a href="#2-2生成数据" class="headerlink" title="2.2生成数据"></a>2.2生成数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(1,30)</span><br><span class="line">y = list(i * 2 + np.random.random()*10 <span class="keyword">for</span> i <span class="keyword">in</span> x)</span><br></pre></td></tr></table></figure>

<h2 id="2-3数据可视化"><a href="#2-3数据可视化" class="headerlink" title="2.3数据可视化"></a>2.3数据可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x,y)</span><br><span class="line">plt</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/1.png"></p>
<h2 id="2-4创建神经网络"><a href="#2-4创建神经网络" class="headerlink" title="2.4创建神经网络"></a>2.4创建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential()<span class="comment">#顺序模型</span></span><br><span class="line">model.add(tf.keras.layers.Dense(1,input_shape=(1,)))<span class="comment">#Dense(输出维度，输入维度)</span></span><br><span class="line">model.summary()<span class="comment">#输出神经网络模型结构</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 1)                 2         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 2</span><br><span class="line">Trainable params: 2</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="2-5编译神经网络"><a href="#2-5编译神经网络" class="headerlink" title="2.5编译神经网络"></a>2.5编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>)<span class="comment">#优化器：adam，其默认学习率：0.001</span></span><br></pre></td></tr></table></figure>

<h2 id="2-6拟合-训练神经网络"><a href="#2-6拟合-训练神经网络" class="headerlink" title="2.6拟合/训练神经网络"></a>2.6拟合/训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span> = model.fit(x,y,epochs=10000,verbose=0)<span class="comment">#其中epochs表示对所有数据训练的次数，verbose为0表示不显示训练过程</span></span><br></pre></td></tr></table></figure>

<h1 id="3-构建多层感知机"><a href="#3-构建多层感知机" class="headerlink" title="3.构建多层感知机"></a>3.构建多层感知机</h1><p>f(x1,x2)=ax1+bx2+c</p>
<h2 id="3-1导入库"><a href="#3-1导入库" class="headerlink" title="3.1导入库"></a>3.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="3-2生成数据"><a href="#3-2生成数据" class="headerlink" title="3.2生成数据"></a>3.2生成数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.arange(1,30)</span><br><span class="line">x2 = list(i * np.random.random()*10 <span class="keyword">for</span> i <span class="keyword">in</span> x1)</span><br><span class="line">y = list(i * 2 + np.random.random()*10 <span class="keyword">for</span> i <span class="keyword">in</span> x1 + x2)</span><br><span class="line">x = np.reshape(np.array([x1,x2]),(-1,2))</span><br></pre></td></tr></table></figure>

<h2 id="3-3数据可视化"><a href="#3-3数据可视化" class="headerlink" title="3.3数据可视化"></a>3.3数据可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x1,y)</span><br></pre></td></tr></table></figure>

<h2 id="3-4创建神经网络"><a href="#3-4创建神经网络" class="headerlink" title="3.4创建神经网络"></a>3.4创建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([tf.keras.layers.Dense(10,input_shape=(2,),activation=<span class="string">&#x27;relu&#x27;</span>),tf.keras.layers.Dense(1)])</span><br><span class="line">model.summary()<span class="comment">#输出模型结构</span></span><br></pre></td></tr></table></figure>

<h2 id="3-5编译神经网络"><a href="#3-5编译神经网络" class="headerlink" title="3.5编译神经网络"></a>3.5编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-6训练神经网络"><a href="#3-6训练神经网络" class="headerlink" title="3.6训练神经网络"></a>3.6训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x,y,epochs=100,verbose=0)</span><br></pre></td></tr></table></figure>

<h1 id="4-实现逻辑回归（二分类）"><a href="#4-实现逻辑回归（二分类）" class="headerlink" title="4.实现逻辑回归（二分类）"></a>4.实现逻辑回归（二分类）</h1><h2 id="4-1导入库"><a href="#4-1导入库" class="headerlink" title="4.1导入库"></a>4.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="4-2生成数据"><a href="#4-2生成数据" class="headerlink" title="4.2生成数据"></a>4.2生成数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.arange(1,30)</span><br><span class="line">x2 = list(i * np.random.random()*10 <span class="keyword">for</span> i <span class="keyword">in</span> x1)</span><br><span class="line">x = np.reshape(np.array([x1,x2]),(-1,2))</span><br><span class="line">y = list(i * 2 + np.random.random()*10 <span class="keyword">for</span> i <span class="keyword">in</span> x1 + x2)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(y)):<span class="comment">#分类指标</span></span><br><span class="line">    y[i] = 1 <span class="keyword">if</span> y[i] &gt; 100 <span class="keyword">else</span> 0</span><br></pre></td></tr></table></figure>

<h2 id="4-3创建神经网络"><a href="#4-3创建神经网络" class="headerlink" title="4.3创建神经网络"></a>4.3创建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(4,input_shape=(2,),activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#第一层隐藏层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(4,activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#第二层隐藏层不需要input_shape，会自动推断</span></span><br><span class="line">model.add(tf.keras.layers.Dense(1,activation=<span class="string">&#x27;sigmoid&#x27;</span>))<span class="comment">#输出层</span></span><br><span class="line">model.summary()<span class="comment">#结构可视化</span></span><br></pre></td></tr></table></figure>

<h2 id="4-4编译神经网络"><a href="#4-4编译神经网络" class="headerlink" title="4.4编译神经网络"></a>4.4编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])<span class="comment">#选择accuracy作为度量标准</span></span><br></pre></td></tr></table></figure>

<h2 id="4-5训练神经网络"><a href="#4-5训练神经网络" class="headerlink" title="4.5训练神经网络"></a>4.5训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span> = model.fit(x,y,epochs=1000,verbose=0)</span><br></pre></td></tr></table></figure>

<h2 id="4-6查看训练过程中损失和准确度曲线"><a href="#4-6查看训练过程中损失和准确度曲线" class="headerlink" title="4.6查看训练过程中损失和准确度曲线"></a>4.6查看训练过程中损失和准确度曲线</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history.history.keys()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回history.history的关键字</span></span><br><span class="line">&gt;&gt; dict_keys([<span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;loss&#x27;</span>))<span class="comment">#loss曲线</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/2.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;accuracy&#x27;</span>))<span class="comment">#acc曲线</span></span><br></pre></td></tr></table></figure>

<h1 id="5-实现"><a href="#5-实现" class="headerlink" title="5.实现"></a>5.实现</h1><p>结合TF自带数据集实现神经网络</p>
<h2 id="5-1-导入库"><a href="#5-1-导入库" class="headerlink" title="5.1 导入库"></a>5.1 导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="5-2-导入TF的FashionMnist数据"><a href="#5-2-导入TF的FashionMnist数据" class="headerlink" title="5.2 导入TF的FashionMnist数据"></a>5.2 导入TF的FashionMnist数据</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是传统MNIST手写字体数据库的加强版（鞋子、衣服、裤子等）</span></span><br><span class="line">(train_image,train_label),(test_image,test_label) = tf.keras.datasets.fashion_mnist.load_data()<span class="comment">#可能涉及翻Q</span></span><br></pre></td></tr></table></figure>

<h2 id="5-3-查看图像数据形状-可视化"><a href="#5-3-查看图像数据形状-可视化" class="headerlink" title="5.3 查看图像数据形状/可视化"></a>5.3 查看图像数据形状/可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_image.shape,train_label.shape</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回结果</span></span><br><span class="line">&gt;&gt; ((60000, 28, 28), (60000,))</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_image[0])<span class="comment">#对图像可视化</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/3.png"></p>
<h2 id="5-4图像数据归一化"><a href="#5-4图像数据归一化" class="headerlink" title="5.4图像数据归一化"></a>5.4图像数据归一化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.max(train_image[0]),np.min(train_image[0])<span class="comment">#查看图像数据值范围</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回结果</span></span><br><span class="line">&gt;&gt; (255, 0)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#最值归一化（0-1）</span></span><br><span class="line">train_image = train_image/255</span><br><span class="line">test_image = test_image/255</span><br></pre></td></tr></table></figure>

<h2 id="5-5创建神经网络"><a href="#5-5创建神经网络" class="headerlink" title="5.5创建神经网络"></a>5.5创建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Flatten(input_shape=(28,28)))<span class="comment">#扁平层==&gt;28*28</span></span><br><span class="line">model.add(tf.keras.layers.Dense(128,activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#隐藏层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(10,activation=<span class="string">&#x27;softmax&#x27;</span>))<span class="comment">#输出层</span></span><br></pre></td></tr></table></figure>

<h2 id="5-6编译神经网络"><a href="#5-6编译神经网络" class="headerlink" title="5.6编译神经网络"></a>5.6编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>数值编码多分类使用：sparse_categorical_crossentropy</p>
<p>Onehot编码多分类使用：categorical_crossentropy</p>
<h2 id="5-7训练神经网络"><a href="#5-7训练神经网络" class="headerlink" title="5.7训练神经网络"></a>5.7训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(train_image,train_label,epochs=10)</span><br></pre></td></tr></table></figure>

<h2 id="5-8验证神经网络"><a href="#5-8验证神经网络" class="headerlink" title="5.8验证神经网络"></a>5.8验证神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(test_image,test_label)<span class="comment">#验证模型</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回结果(损失值loss，准确度accuracy)</span></span><br><span class="line">&gt;&gt; [0.33301281309127806, 0.883]</span><br></pre></td></tr></table></figure>

<h2 id="5-9-对标签Onehot处理"><a href="#5-9-对标签Onehot处理" class="headerlink" title="5.9 对标签Onehot处理"></a>5.9 对标签Onehot处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_label_onehot = tf.keras.utils.to_categorical(train_label)</span><br><span class="line">test_label_onehot = tf.keras.utils.to_categorical(test_label)</span><br><span class="line">train_label_onehot</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回结果</span></span><br><span class="line">array([[0., 0., 0., ..., 0., 0., 1.],</span><br><span class="line">       [1., 0., 0., ..., 0., 0., 0.],</span><br><span class="line">       [1., 0., 0., ..., 0., 0., 0.],</span><br><span class="line">       ...,</span><br><span class="line">       [0., 0., 0., ..., 0., 0., 0.],</span><br><span class="line">       [1., 0., 0., ..., 0., 0., 0.],</span><br><span class="line">       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</span><br></pre></td></tr></table></figure>

<h2 id="5-10创建、编译、训练、验证神经网络"><a href="#5-10创建、编译、训练、验证神经网络" class="headerlink" title="5.10创建、编译、训练、验证神经网络"></a>5.10创建、编译、训练、验证神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(tf.keras.layers.Flatten(input_shape=(28,28)))<span class="comment">#扁平层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(128,activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#隐藏层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(10,activation=<span class="string">&#x27;softmax&#x27;</span>))<span class="comment">#输出层</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">history</span> = model.fit(train_image,train_label_onehot,epochs=10,validation_data=(test_image,test_label_onehot))</span><br><span class="line"></span><br><span class="line">model.evaluate(test_image,test_label_onehot)<span class="comment">#验证模型</span></span><br></pre></td></tr></table></figure>

<h2 id="5-11查看训练过程损失和准确度曲线"><a href="#5-11查看训练过程损失和准确度曲线" class="headerlink" title="5.11查看训练过程损失和准确度曲线"></a>5.11查看训练过程损失和准确度曲线</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">history.history.keys()</span><br><span class="line"><span class="comment">#损失值曲线</span></span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;loss&#x27;</span>),label=<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;val_loss&#x27;</span>),label=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/4.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#准确度曲线</span></span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;accuracy&#x27;</span>),label=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;val_accuracy&#x27;</span>),label=<span class="string">&#x27;val_accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/5.png"></p>
<h1 id="5-12神经网络中加入dropout训练"><a href="#5-12神经网络中加入dropout训练" class="headerlink" title="5.12神经网络中加入dropout训练"></a>5.12神经网络中加入dropout训练</h1><p>训练过程中随机丢弃部分神经单元，测试不丢弃，避免神经网络过拟合，减少神经元之间复杂共适应关系，dropout导致两个神经元不一定每次都在一个dropout网络出现，权值更新不再依赖固定关系的隐含节点的共同作用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建神经网络</span></span><br><span class="line">model = tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Flatten(input_shape=(28,28)))<span class="comment">#扁平层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(128,activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#隐藏层</span></span><br><span class="line">model.add(tf.keras.layers.Dropout(0.2))<span class="comment">#Dropout层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(128,activation=<span class="string">&#x27;relu&#x27;</span>))<span class="comment">#隐藏层</span></span><br><span class="line">model.add(tf.keras.layers.Dropout(0.2))<span class="comment">#Dropout层</span></span><br><span class="line">model.add(tf.keras.layers.Dense(10,activation=<span class="string">&#x27;softmax&#x27;</span>))<span class="comment">#输出层</span></span><br><span class="line"><span class="comment">#编译神经网络</span></span><br><span class="line">model.compile(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="comment">#训练神经网络</span></span><br><span class="line"><span class="built_in">history</span> = model.fit(train_image,train_label_onehot,epochs=10,validation_data=(test_image,test_label_onehot))</span><br><span class="line"><span class="comment">#训练过程可视化</span></span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;accuracy&#x27;</span>),label=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">plt.plot(history.epoch,history.history.get(<span class="string">&#x27;val_accuracy&#x27;</span>),label=<span class="string">&#x27;val_accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<h1 id="6-函数式API搭建神经网络"><a href="#6-函数式API搭建神经网络" class="headerlink" title="6.函数式API搭建神经网络"></a>6.函数式API搭建神经网络</h1><h2 id="6-1导入库"><a href="#6-1导入库" class="headerlink" title="6.1导入库"></a>6.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="6-2数据集预处理"><a href="#6-2数据集预处理" class="headerlink" title="6.2数据集预处理"></a>6.2数据集预处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">(train_image,train_label),(test_image,test_label) = tf.keras.datasets.fashion_mnist.load_data()</span><br><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line">train_image = train_image/255</span><br><span class="line">test_image = test_image/255</span><br><span class="line"><span class="comment"># 数据形状</span></span><br><span class="line">train_image.shape,train_label.shape</span><br><span class="line"><span class="comment"># ((60000, 28, 28), (60000,))</span></span><br></pre></td></tr></table></figure>

<h2 id="6-3搭建神经网络"><a href="#6-3搭建神经网络" class="headerlink" title="6.3搭建神经网络"></a>6.3搭建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单层网络搭建</span></span><br><span class="line">input = keras.Input(shape=(28,28))</span><br><span class="line">x = keras.layers.Flatten()(input)</span><br><span class="line">x = keras.layers.Dense(32,activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">x = keras.layers.Dropout(0.5)(x)</span><br><span class="line">x = keras.layers.Dense(64)(x)</span><br><span class="line">output = keras.layers.Dense(10,activation=<span class="string">&#x27;softmax&#x27;</span>)(x)</span><br><span class="line"><span class="comment"># 确定输入输出</span></span><br><span class="line">model = keras.Model(inputs=input,outputs=output)</span><br><span class="line"><span class="comment"># 网络结构可视化</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<h2 id="6-4-编译、训练神经网络"><a href="#6-4-编译、训练神经网络" class="headerlink" title="6.4 编译、训练神经网络"></a>6.4 编译、训练神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_image,train_label,epochs=<span class="number">10</span>,batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>

<h1 id="7-基于MNIST构建卷积神经网络"><a href="#7-基于MNIST构建卷积神经网络" class="headerlink" title="7.基于MNIST构建卷积神经网络"></a>7.基于MNIST构建卷积神经网络</h1><h2 id="7-1导入库"><a href="#7-1导入库" class="headerlink" title="7.1导入库"></a>7.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br><span class="line">from tensorflow.keras import layers</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure>

<h2 id="7-2-数据预处理"><a href="#7-2-数据预处理" class="headerlink" title="7.2 数据预处理"></a>7.2 数据预处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line">x_train = x_train.reshape((-1,28,28,1))</span><br><span class="line">x_test = x_test.reshape((-1,28,28,1))</span><br></pre></td></tr></table></figure>

<h2 id="7-3-搭建神经网络"><a href="#7-3-搭建神经网络" class="headerlink" title="7.3 搭建神经网络"></a>7.3 搭建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">model.add(layers.Conv2D(input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]), filters=32, kernel_size=(3,3), strides=(1,1), padding=<span class="string">&#x27;valid&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment"># 池化层</span></span><br><span class="line">model.add(layers.MaxPool2D(pool_size=(2,2)))</span><br><span class="line"><span class="comment"># 平坦层</span></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">model.add(layers.Dense(32, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">model.add(layers.Dense(10, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h2 id="7-4-编译神经网络"><a href="#7-4-编译神经网络" class="headerlink" title="7.4 编译神经网络"></a>7.4 编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.SparseCategoricalCrossentropy(),metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>其中loss=keras.losses.CategoricalCrossentropy(), 需要使用to_categorical操作，即Onehot预处理</p>
<h2 id="7-5-结构可视化"><a href="#7-5-结构可视化" class="headerlink" title="7.5 结构可视化"></a>7.5 结构可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回结果</span></span><br><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">conv2d (Conv2D)              (None, 26, 26, 32)        320       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, 5408)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, 32)                173088    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 10)                330       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 173,738</span><br><span class="line">Trainable params: 173,738</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="7-6训练网络和训练过程可视化"><a href="#7-6训练网络和训练过程可视化" class="headerlink" title="7.6训练网络和训练过程可视化"></a>7.6训练网络和训练过程可视化</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练网络</span></span><br><span class="line"><span class="built_in">history</span> = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.1)</span><br><span class="line"><span class="comment"># 过程可视化</span></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_accuracy&#x27;</span>])</span><br><span class="line">plt.legend([<span class="string">&#x27;training&#x27;</span>, <span class="string">&#x27;valivation&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/6.png"></p>
<h2 id="7-7评估神经网络"><a href="#7-7评估神经网络" class="headerlink" title="7.7评估神经网络"></a>7.7评估神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回结果</span></span><br><span class="line">&gt;&gt; 	10000/10000 [====] - 1s 80us/sample - loss: 0.0986 - accuracy: 0.9753</span><br></pre></td></tr></table></figure>

<h2 id="7-8实现预测"><a href="#7-8实现预测" class="headerlink" title="7.8实现预测"></a>7.8实现预测</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction &#x3D; model.predict(x_test)</span><br></pre></td></tr></table></figure>



<h1 id="8-基于IMDB构建循环神经网络RNN"><a href="#8-基于IMDB构建循环神经网络RNN" class="headerlink" title="8.基于IMDB构建循环神经网络RNN"></a>8.基于IMDB构建循环神经网络RNN</h1><h2 id="8-1导入库"><a href="#8-1导入库" class="headerlink" title="8.1导入库"></a>8.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br><span class="line">from tensorflow.keras import layers</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">tf.__version__</span><br></pre></td></tr></table></figure>

<h2 id="8-2数据预处理"><a href="#8-2数据预处理" class="headerlink" title="8.2数据预处理"></a>8.2数据预处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_words = 30000</span><br><span class="line">maxlen = 200</span><br><span class="line">(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)</span><br><span class="line">x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape, <span class="string">&#x27; &#x27;</span>, y_train.shape)<span class="comment"># (25000, 200)   (25000,)</span></span><br><span class="line"><span class="built_in">print</span>(x_test.shape, <span class="string">&#x27; &#x27;</span>, y_test.shape)<span class="comment"># (25000, 200)   (25000,)</span></span><br></pre></td></tr></table></figure>

<h2 id="8-3构建神经网络"><a href="#8-3构建神经网络" class="headerlink" title="8.3构建神经网络"></a>8.3构建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">        layers.Embedding(input_dim=30000, output_dim=32, input_length=maxlen),</span><br><span class="line">        layers.SimpleRNN(32, return_sequences=True),</span><br><span class="line">        layers.SimpleRNN(1, activation=<span class="string">&#x27;sigmoid&#x27;</span>, return_sequences=False)</span><br><span class="line">])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回结果</span></span><br><span class="line">Model: <span class="string">&quot;sequential&quot;</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                 Output Shape              Param <span class="comment">#   </span></span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 200, 32)           960000    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">simple_rnn (SimpleRNN)       (None, 200, 32)           2080      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">simple_rnn_1 (SimpleRNN)     (None, 1)                 34        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 962,114</span><br><span class="line">Trainable params: 962,114</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>

<h2 id="8-4编译神经网络"><a href="#8-4编译神经网络" class="headerlink" title="8.4编译神经网络"></a>8.4编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.BinaryCrossentropy(),metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="8-5训练神经网络"><a href="#8-5训练神经网络" class="headerlink" title="8.5训练神经网络"></a>8.5训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time<span class="comment">#计算该小模块运算时间：结果返回 Wall time: 2min 37s</span></span><br><span class="line"><span class="built_in">history</span> = model.fit(x_train, y_train, batch_size=64, epochs=5,validation_split=0.1)</span><br></pre></td></tr></table></figure>

<h2 id="8-6绘制训练过程的准确度曲线"><a href="#8-6绘制训练过程的准确度曲线" class="headerlink" title="8.6绘制训练过程的准确度曲线"></a>8.6绘制训练过程的准确度曲线</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_accuracy&#x27;</span>])</span><br><span class="line">plt.legend([<span class="string">&#x27;training&#x27;</span>, <span class="string">&#x27;valivation&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/7.png"></p>
<h1 id="9-基于IMDB构建长短期记忆神经网络LSTM"><a href="#9-基于IMDB构建长短期记忆神经网络LSTM" class="headerlink" title="9.基于IMDB构建长短期记忆神经网络LSTM"></a>9.基于IMDB构建长短期记忆神经网络LSTM</h1><h2 id="9-1导入库"><a href="#9-1导入库" class="headerlink" title="9.1导入库"></a>9.1导入库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br><span class="line">from tensorflow.keras import layers</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">tf.__version__</span><br></pre></td></tr></table></figure>

<h2 id="9-2数据预处理"><a href="#9-2数据预处理" class="headerlink" title="9.2数据预处理"></a>9.2数据预处理</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_words = 30000</span><br><span class="line">maxlen = 200</span><br><span class="line">(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)</span><br><span class="line">x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen, padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape, <span class="string">&#x27; &#x27;</span>, y_train.shape)<span class="comment"># (25000, 200)   (25000,)</span></span><br><span class="line"><span class="built_in">print</span>(x_test.shape, <span class="string">&#x27; &#x27;</span>, y_test.shape)<span class="comment"># (25000, 200)   (25000,)</span></span><br></pre></td></tr></table></figure>

<h2 id="9-3构建神经网络"><a href="#9-3构建神经网络" class="headerlink" title="9.3构建神经网络"></a>9.3构建神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">        layers.Embedding(input_dim=30000, output_dim=32, input_length=maxlen),</span><br><span class="line">        layers.LSTM(32, return_sequences=True),</span><br><span class="line">        layers.LSTM(1, activation=<span class="string">&#x27;sigmoid&#x27;</span>, return_sequences=False)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<h2 id="9-4编译神经网络"><a href="#9-4编译神经网络" class="headerlink" title="9.4编译神经网络"></a>9.4编译神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.BinaryCrossentropy(),metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="9-5训练神经网络"><a href="#9-5训练神经网络" class="headerlink" title="9.5训练神经网络"></a>9.5训练神经网络</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"><span class="built_in">history</span> = model.fit(x_train, y_train, batch_size=64, epochs=5,validation_split=0.1)</span><br></pre></td></tr></table></figure>

<h2 id="9-6绘制训练过程的准确度曲线"><a href="#9-6绘制训练过程的准确度曲线" class="headerlink" title="9.6绘制训练过程的准确度曲线"></a>9.6绘制训练过程的准确度曲线</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_accuracy&#x27;</span>])</span><br><span class="line">plt.legend([<span class="string">&#x27;training&#x27;</span>, <span class="string">&#x27;valivation&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt</span><br></pre></td></tr></table></figure>

<p><img src="/images/tf2/8.png"></p>
<h1 id="10-Tensorflow基本语法"><a href="#10-Tensorflow基本语法" class="headerlink" title="10.Tensorflow基本语法"></a>10.Tensorflow基本语法</h1><h2 id="10-1-numpy-ndarray和tensor互转"><a href="#10-1-numpy-ndarray和tensor互转" class="headerlink" title="10.1 numpy.ndarray和tensor互转"></a>10.1 numpy.ndarray和tensor互转</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy.ndarray转tensor</span></span><br><span class="line">x = tf.convert_to_tensor(x,dtype=tf.float32)</span><br><span class="line"><span class="comment"># tensor转numpy</span></span><br><span class="line">x = x.numpy()</span><br><span class="line"><span class="comment"># 判断变量是否为tensor</span></span><br><span class="line">tf.is_tensor(x)</span><br><span class="line"><span class="comment"># 数据类型转换</span></span><br><span class="line">x = tf.cast(x,dtype=tf.int32)</span><br></pre></td></tr></table></figure>

<h2 id="10-2-查看tensor形状和类型"><a href="#10-2-查看tensor形状和类型" class="headerlink" title="10.2 查看tensor形状和类型"></a>10.2 查看tensor形状和类型</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.shape <span class="comment"># (TensorShape([60000, 28, 28])</span></span><br><span class="line">x.dtype <span class="comment"># tf.float32</span></span><br></pre></td></tr></table></figure>

<h2 id="10-3-将数据生成指定长度batch的迭代器"><a href="#10-3-将数据生成指定长度batch的迭代器" class="headerlink" title="10.3 将数据生成指定长度batch的迭代器"></a>10.3 将数据生成指定长度batch的迭代器</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 128长度的batch</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)</span><br><span class="line"><span class="comment"># 转为迭代器类型</span></span><br><span class="line">train_iter = iter(train_db)</span><br><span class="line">sample = next(train_iter)</span><br></pre></td></tr></table></figure>

<h2 id="10-4-定义tensor变量（初始化w和b）"><a href="#10-4-定义tensor变量（初始化w和b）" class="headerlink" title="10.4 定义tensor变量（初始化w和b）"></a>10.4 定义tensor变量（初始化w和b）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.zeros([256]))</span><br><span class="line">w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.zeros([128]))</span><br><span class="line">w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))</span><br><span class="line">b3 = tf.Variable(tf.zeros([10]))</span><br></pre></td></tr></table></figure>

<h2 id="10-5-生成tensor变量-常量"><a href="#10-5-生成tensor变量-常量" class="headerlink" title="10.5 生成tensor变量/常量"></a>10.5 生成tensor变量/常量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正态分布的tensor：从服从指定正态分布的数值中取出指定个数的值，均值mean，标准差stddev</span></span><br><span class="line">x= tf.random.normal([4,25,8],mean=0.0,stddev=1.0)</span><br><span class="line"><span class="comment"># 截断正态分布的tensor：从服从指定正态分布的数值中取出指定个数的值，均值mean，标准差stddev，且保留[mean-2×stddev,mean+2×stddev]范围内的随机数</span></span><br><span class="line">x= tf.random.truncated_normal([4,25,8],mean=0.0,stddev=1.0)</span><br><span class="line"><span class="comment"># 生成全0的tensor</span></span><br><span class="line">tf.zeros([2, 3], tf.int32)</span><br><span class="line"><span class="comment"># 生成全1的tensor</span></span><br><span class="line">tf.ones([2, 3], tf.int32)</span><br><span class="line"><span class="comment"># 生成形状相同的tensor</span></span><br><span class="line">tensor=[[1, 2, 3], [4, 5, 6]] </span><br><span class="line">x1 = tf.ones_like(tensor)</span><br><span class="line">x2 = tf.zeros_like(tensor)</span><br><span class="line"><span class="comment"># 生成指定值的tensor</span></span><br><span class="line">x = tf.fill([2,2],2)</span><br><span class="line"><span class="comment"># 生成常量</span></span><br><span class="line">x = tf.constant([1,2,3,4,5,6],shape=[3,2])</span><br><span class="line"><span class="comment"># 定义递增tensor</span></span><br><span class="line">x = tf.range(5)</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=328, shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="10-6-维度变换"><a href="#10-6-维度变换" class="headerlink" title="10.6 维度变换"></a>10.6 维度变换</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加tensor维度</span></span><br><span class="line">x.shape <span class="comment"># (4, 25, 8)</span></span><br><span class="line">tf.expand_dims(x,axis=0).shape <span class="comment">#添加第一维度,TensorShape([1, 4, 25, 8])</span></span><br><span class="line"><span class="comment"># tensor维度交换——转置</span></span><br><span class="line">tf.transpose(x).shape</span><br><span class="line"><span class="comment"># tensor维度交换——指定维度间转置（将第一维度和第二维度互换）</span></span><br><span class="line">tf.transpose(a,perm=[1,0,2])</span><br><span class="line"><span class="comment"># 减少tensor维度</span></span><br><span class="line"><span class="comment"># 默认第一维</span></span><br><span class="line">tf.squeeze(x)</span><br><span class="line"><span class="comment"># 指定维度（删除第三维）</span></span><br><span class="line">tf.squeeze(b,axis=2)</span><br><span class="line"><span class="comment"># 指定扩展维度</span></span><br><span class="line">x = tf.ones([3,4])</span><br><span class="line">tf.broadcast_to(x,[2,3,4])</span><br><span class="line"><span class="comment"># 复制维度</span></span><br><span class="line">x.shape <span class="comment"># (1,3,4)</span></span><br><span class="line">x = tf.tile(x,[2,1,1])<span class="comment"># 将第一维度复制2遍</span></span><br><span class="line">x.shape <span class="comment"># (2,3,4)</span></span><br></pre></td></tr></table></figure>

<h2 id="10-7-broadcasting机制"><a href="#10-7-broadcasting机制" class="headerlink" title="10.7 broadcasting机制"></a>10.7 broadcasting机制</h2><p>通常在tensor相加采取broadcasting机制，即从右往左匹配，维度不够则复制对应维数，例如：</p>
<p>[4,16,16,32] + [32] 出现维度不匹配</p>
<p>① [32] →[1,1,1,32] → [4,16,16,32]</p>
<p>② [4,16,16,32] + [4,16,16,32] 成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([4,32,32,3])</span><br><span class="line">y = tf.random.normal([3])</span><br><span class="line">(x+y).shape <span class="comment"># TensorShape([(4), (32), (32), (3)])</span></span><br></pre></td></tr></table></figure>

<h2 id="10-8-tensor内部运算符"><a href="#10-8-tensor内部运算符" class="headerlink" title="10.8 tensor内部运算符"></a>10.8 tensor内部运算符</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.math.log(x)<span class="comment">#取对数</span></span><br><span class="line">x = tf.sqrt(x)<span class="comment">#开根号</span></span><br><span class="line">x = tf.square(x)<span class="comment">#取平方</span></span><br><span class="line">val = tf.reduce_sum(x)<span class="comment">#求和</span></span><br><span class="line">val = tf.norm()<span class="comment">#求欧基里得范数</span></span><br><span class="line">val = tf.reduce_mean(x)<span class="comment">#求均值</span></span><br><span class="line">min = tf.reduce_min(x)<span class="comment">#最小值</span></span><br><span class="line">max = tf.reduce_max(x)<span class="comment">#最大值</span></span><br><span class="line">index = tf.argmin(x)<span class="comment">#求最小值索引</span></span><br><span class="line">index = tf.argmax(x)<span class="comment">#求最大值索引</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="10-9-矩阵相乘"><a href="#10-9-矩阵相乘" class="headerlink" title="10.9 矩阵相乘"></a>10.9 矩阵相乘</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([2,2])</span><br><span class="line">b = tf.fill([2,2],3.0)</span><br><span class="line">a@b <span class="comment"># 方式一</span></span><br><span class="line">tf.matmul(a,b)<span class="comment"># 方式二</span></span><br></pre></td></tr></table></figure>

<h2 id="10-10-tensor合并"><a href="#10-10-tensor合并" class="headerlink" title="10.10 tensor合并"></a>10.10 tensor合并</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([4,35,8])</span><br><span class="line">b = tf.ones([2,35,8])</span><br><span class="line">c = tf.concat([a,b],axis=0)</span><br><span class="line">c.shape <span class="comment"># TensorShape([6, 35, 8])</span></span><br></pre></td></tr></table></figure>

<h2 id="10-11-定义tensor静态量"><a href="#10-11-定义tensor静态量" class="headerlink" title="10.11 定义tensor静态量"></a>10.11 定义tensor静态量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([1,2,3,4],dtype=tf.float32)</span><br></pre></td></tr></table></figure>

<h2 id="10-12-判断tensor对应元素是否相等"><a href="#10-12-判断tensor对应元素是否相等" class="headerlink" title="10.12 判断tensor对应元素是否相等"></a>10.12 判断tensor对应元素是否相等</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.equal(a,b)</span><br></pre></td></tr></table></figure>

<h2 id="10-13-在一维张量中找到唯一的元素"><a href="#10-13-在一维张量中找到唯一的元素" class="headerlink" title="10.13 在一维张量中找到唯一的元素"></a>10.13 在一维张量中找到唯一的元素</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([4,4,1,2,3])</span><br><span class="line">tf.unique(x)<span class="comment"># [0, 0, 1, 2, 3]</span></span><br></pre></td></tr></table></figure>

<h1 id="11-模型子类化"><a href="#11-模型子类化" class="headerlink" title="11.模型子类化"></a>11.模型子类化</h1><p>模型子类化中，在init方法中定义网络结构，在 call 方法中定义前向传播，在compute_output_shape方法中定义输出维度</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self, num_classes=10):</span><br><span class="line">        super(MyModel, self).__init__(name=<span class="string">&#x27;my_model&#x27;</span>)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.layer1 = layers.Dense(32, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.layer2 = layers.Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        h1 = self.layer1(inputs)</span><br><span class="line">        out = self.layer2(h1)</span><br><span class="line">        <span class="built_in">return</span> out</span><br><span class="line"></span><br><span class="line">    def compute_output_shape(self, input_shape):</span><br><span class="line">        shape = tf.TensorShape(input_shape).as_list()</span><br><span class="line">        shape[-1] = self.num_classes</span><br><span class="line">        <span class="built_in">return</span> tf.TensorShape(shape)</span><br><span class="line"></span><br><span class="line">model = MyModel(num_classes=10)</span><br><span class="line">model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),</span><br><span class="line">             loss=tf.keras.losses.categorical_crossentropy,</span><br><span class="line">             metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.fit(train_x, train_y, batch_size=16, epochs=5)</span><br></pre></td></tr></table></figure>

<h1 id="12-保存和读取模型"><a href="#12-保存和读取模型" class="headerlink" title="12.保存和读取模型"></a>12.保存和读取模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model.save(&#39;model_save.h5&#39;)</span><br><span class="line">del model</span><br><span class="line">model &#x3D; keras.models.load_model(&#39;model_save.h5&#39;)</span><br><span class="line"></span><br><span class="line"># 保存json格式参数</span><br><span class="line">json_string &#x3D; model.to_json()</span><br><span class="line">print(json_string)</span><br><span class="line"># 写入文件</span><br><span class="line">with open(&quot;model.json&quot;, &quot;w&quot;) as json_file:</span><br><span class="line">    json_file.write(json_string)</span><br><span class="line"># 加载和使用</span><br><span class="line"># reinitialized_model &#x3D; keras.models.model_from_json(json_string)</span><br><span class="line">new_prediction &#x3D; reinitialized_model.predict(x_test)</span><br><span class="line"></span><br><span class="line"># 保存网络结构</span><br><span class="line">config &#x3D; model.get_config()</span><br><span class="line"># 加载和使用</span><br><span class="line">reinitialized_model &#x3D; keras.Model.from_config(config)</span><br><span class="line">new_prediction &#x3D; reinitialized_model.predict(x_test)</span><br><span class="line"></span><br><span class="line"># 保存网络参数</span><br><span class="line">weights &#x3D; model.get_weights()</span><br><span class="line">model.set_weights(weights)</span><br></pre></td></tr></table></figure>

<h1 id="13-自编码器"><a href="#13-自编码器" class="headerlink" title="13.自编码器"></a>13.自编码器</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">encode_input = keras.Input(shape=(28,28,1), name=<span class="string">&#x27;src_img&#x27;</span>)</span><br><span class="line">h1 = layers.Conv2D(16, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(encode_input)</span><br><span class="line">h1 = layers.Conv2D(32, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">h1 = layers.MaxPool2D(3)(h1)</span><br><span class="line">h1 = layers.Conv2D(32, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">h1 = layers.Conv2D(16, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">encode_output = layers.GlobalMaxPool2D()(h1)</span><br><span class="line"></span><br><span class="line">encode_model = keras.Model(inputs=encode_input, outputs=encode_output, name=<span class="string">&#x27;encoder&#x27;</span>)</span><br><span class="line">encode_model.summary()</span><br><span class="line"></span><br><span class="line">decode_input = keras.Input(shape=(16,), name=<span class="string">&#x27;encoded_img&#x27;</span>)</span><br><span class="line">h2 = layers.Reshape((4, 4, 1))(decode_input)</span><br><span class="line">h2 = layers.Conv2DTranspose(16, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h2)</span><br><span class="line">h2 = layers.Conv2DTranspose(32, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h2)</span><br><span class="line">h2 = layers.UpSampling2D(3)(h2)</span><br><span class="line">h2 = layers.Conv2DTranspose(16, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h2)</span><br><span class="line">decode_output = layers.Conv2DTranspose(1, 3, activation=<span class="string">&#x27;relu&#x27;</span>)(h2)</span><br><span class="line">decode_model = keras.Model(inputs=decode_input, outputs=decode_output, name=<span class="string">&#x27;decoder&#x27;</span>)</span><br><span class="line">decode_model.summary()</span><br><span class="line"></span><br><span class="line">autoencoder_input = keras.Input(shape=(28,28,1), name=<span class="string">&#x27;img&#x27;</span>)</span><br><span class="line">h3 = encode_model(autoencoder_input)</span><br><span class="line">autoencoder_output = decode_model(h3)</span><br><span class="line">autoencoder = keras.Model(inputs=autoencoder_input, outputs=autoencoder_output,name=<span class="string">&#x27;autoencoder&#x27;</span>)</span><br><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure>



<h1 id="14-多输入与多输出网络"><a href="#14-多输入与多输出网络" class="headerlink" title="14.多输入与多输出网络"></a>14.多输入与多输出网络</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个根据文档内容、标签和标题，预测文档优先级和执行部门的网络</span></span><br><span class="line"><span class="comment"># 超参</span></span><br><span class="line">num_words = 2000</span><br><span class="line">num_tags = 12</span><br><span class="line">num_departments = 4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入</span></span><br><span class="line">body_input = keras.Input(shape=(None,), name=<span class="string">&#x27;body&#x27;</span>)</span><br><span class="line">title_input = keras.Input(shape=(None,), name=<span class="string">&#x27;title&#x27;</span>)</span><br><span class="line">tag_input = keras.Input(shape=(num_tags,), name=<span class="string">&#x27;tag&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入层</span></span><br><span class="line">body_feat = layers.Embedding(num_words, 64)(body_input)</span><br><span class="line">title_feat = layers.Embedding(num_words, 64)(title_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征提取层</span></span><br><span class="line">body_feat = layers.LSTM(32)(body_feat)</span><br><span class="line">title_feat = layers.LSTM(128)(title_feat)</span><br><span class="line">features = layers.concatenate([title_feat,body_feat, tag_input])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类层</span></span><br><span class="line">priority_pred = layers.Dense(1, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name=<span class="string">&#x27;priority&#x27;</span>)(features)</span><br><span class="line">department_pred = layers.Dense(num_departments, activation=<span class="string">&#x27;softmax&#x27;</span>, name=<span class="string">&#x27;department&#x27;</span>)(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">model = keras.Model(inputs=[body_input, title_input, tag_input],</span><br><span class="line">                    outputs=[priority_pred, department_pred])</span><br><span class="line">model.summary()</span><br><span class="line">keras.utils.plot_model(model, <span class="string">&#x27;multi_model.png&#x27;</span>, show_shapes=True)<span class="comment">#实现个网络层的输入/输出形状</span></span><br><span class="line">model.compile(optimizer=keras.optimizers.RMSprop(1e-3),</span><br><span class="line">             loss=&#123;<span class="string">&#x27;priority&#x27;</span>: <span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;department&#x27;</span>: <span class="string">&#x27;categorical_crossentropy&#x27;</span>&#125;,</span><br><span class="line">             loss_weights=[1., 0.2])<span class="comment">#两个输出两个loss权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入输入数据</span></span><br><span class="line">title_data = np.random.randint(num_words, size=(1280, 10))</span><br><span class="line">body_data = np.random.randint(num_words, size=(1280, 100))</span><br><span class="line">tag_data = np.random.randint(2, size=(1280, num_tags)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"><span class="comment"># 标签</span></span><br><span class="line">priority_label = np.random.random(size=(1280, 1))</span><br><span class="line">department_label = np.random.randint(2, size=(1280, num_departments))</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="built_in">history</span> = model.fit(</span><br><span class="line">    &#123;<span class="string">&#x27;title&#x27;</span>: title_data, <span class="string">&#x27;body&#x27;</span>:body_data, <span class="string">&#x27;tag&#x27;</span>:tag_data&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;priority&#x27;</span>:priority_label, <span class="string">&#x27;department&#x27;</span>:department_label&#125;,</span><br><span class="line">    batch_size=32,</span><br><span class="line">    epochs=5</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>其中执行keras.utils.plot_model(model, ‘multi_model.png’, show_shapes=True)可能会遇到以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: Failed to import pydot. You must install pydot and graphviz for &#96;pydotprint&#96; to work.</span><br></pre></td></tr></table></figure>

<p>解决方法：</p>
<p>1.pip install pydot and graphviz</p>
<p>2.下载graphviz.msi：<a target="_blank" rel="noopener" href="https://graphviz.gitlab.io/_pages/Download/windows/graphviz-2.38.msi">https://graphviz.gitlab.io/_pages/Download/windows/graphviz-2.38.msi</a></p>
<p>3.在上述代码前添加，路径基于graphviz的安装地址</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;PATH&quot;</span>] += os.pathsep + <span class="string">&#x27;C:/Program Files (x86)/Graphviz2.38/bin/&#x27;</span></span><br></pre></td></tr></table></figure>

<p>参考图：</p>
<p><img src="/images/tf2/9.png"></p>
<h1 id="15-共享网络层"><a href="#15-共享网络层" class="headerlink" title="15.共享网络层"></a>15.共享网络层</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">share_embedding = layers.Embedding(<span class="number">1000</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">input1 = keras.Input(shape=(<span class="literal">None</span>,), dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">input2 = keras.Input(shape=(<span class="literal">None</span>,), dtype=<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">feat1 = share_embedding(input1)</span><br><span class="line">feat2 = share_embedding(input2)</span><br></pre></td></tr></table></figure>

<h1 id="16-自定义网络层"><a href="#16-自定义网络层" class="headerlink" title="16.自定义网络层"></a>16.自定义网络层</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PART I</span></span><br><span class="line"><span class="comment"># 自定义构建全连接层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units=<span class="number">32</span></span>):</span><span class="comment"># 定义结构参数（全连接层对应神经元数量即可）</span></span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span><span class="comment"># 定义/初始化参数w和b（随机生成且可训练调整）</span></span><br><span class="line">        self.w = self.add_weight(shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">                                 initializer=<span class="string">&#x27;random_normal&#x27;</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">        self.b = self.add_weight(shape=(self.units,),</span><br><span class="line">                                 initializer=<span class="string">&#x27;random_normal&#x27;</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span><span class="comment"># 定义前向传播</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span>(<span class="params">self</span>):</span> <span class="comment"># 模型的配置参数（全连接层仅包含神经元数量）</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;units&#x27;</span>: self.units&#125;</span><br><span class="line"></span><br><span class="line">inputs = keras.Input((<span class="number">4</span>,))<span class="comment"># 输入</span></span><br><span class="line">outputs = MyDense(<span class="number">10</span>)(inputs)<span class="comment"># 输出</span></span><br><span class="line">model = keras.Model(inputs, outputs)<span class="comment"># 模型（输入，输出）</span></span><br><span class="line">config = model.get_config()<span class="comment"># 模型配置参数</span></span><br><span class="line">new_model = keras.Model.from_config(config, custom_objects=&#123;<span class="string">&#x27;MyDense&#x27;</span>:MyDense&#125;)<span class="comment"># 模型读取配置参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PART II</span></span><br><span class="line"><span class="comment"># 在自定义网络层调用其他网络层，自定义RNN中调用Dense层</span></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">time_step = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">hidden_dim = <span class="number">32</span></span><br><span class="line">inputs_dim = <span class="number">5</span></span><br><span class="line"><span class="comment"># 自定义循环神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRnn</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyRnn, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim <span class="comment">#隐藏层维度</span></span><br><span class="line">        self.projection1 = layers.Dense(units=hidden_dim, activation=<span class="string">&#x27;relu&#x27;</span>) <span class="comment">#全连接层神经元数量</span></span><br><span class="line">        self.projection2 = layers.Dense(units=hidden_dim, activation=<span class="string">&#x27;relu&#x27;</span>) <span class="comment">#全连接层神经元数量</span></span><br><span class="line">        self.classifier = layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>) <span class="comment">#输出层维度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        outs = []</span><br><span class="line">        states = tf.zeros(shape=[inputs.shape[<span class="number">0</span>], self.hidden_dim])</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(inputs.shape[<span class="number">1</span>]):</span><br><span class="line">            x = inputs[:,t,:]</span><br><span class="line">            h = self.projection1(x)</span><br><span class="line">            y = h + self.projection2(states)</span><br><span class="line">            states = y</span><br><span class="line">            outs.append(y)</span><br><span class="line">        <span class="comment"># print(outs)</span></span><br><span class="line">        features = tf.stack(outs, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(features.shape)</span><br><span class="line">        <span class="keyword">return</span> self.classifier(features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line">inputs = keras.Input(batch_shape=(batch_size, time_step, inputs_dim))</span><br><span class="line">x = layers.Conv1D(<span class="number">32</span>, <span class="number">3</span>)(inputs)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">outputs = MyRnn()(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line"></span><br><span class="line">rnn_model = MyRnn()</span><br><span class="line">_ = rnn_model(tf.zeros((<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>)))</span><br></pre></td></tr></table></figure>

<h1 id="17-自定义损失和指标"><a href="#17-自定义损失和指标" class="headerlink" title="17.自定义损失和指标"></a>17.自定义损失和指标</h1><p>自定义指标只需继承Metric类， 并重写一下函数_init_(self)，初始化。</p>
<p>update_state(self，y_true，y_pred，sample_weight = None)，它使用目标y_true和模型预测y_pred来更新状态变量。</p>
<p>result(self)，它使用状态变量来计算最终结果。</p>
<p>reset_states(self)，重新初始化度量的状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现CatgoricalTruePositives指标：计算正确分类为属于给定类的样本数量</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CatgoricalTruePostives</span>(<span class="params">keras.metrics.Metric</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name=<span class="string">&#x27;binary_true_postives&#x27;</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CatgoricalTruePostives, self).__init__(name=name, **kwargs)</span><br><span class="line">        self.true_postives = self.add_weight(name=<span class="string">&#x27;tp&#x27;</span>, initializer=<span class="string">&#x27;zeros&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_state</span>(<span class="params">self, y_true, y_pred, sample_weight=<span class="literal">None</span></span>):</span></span><br><span class="line">        y_pred = tf.argmax(y_pred)</span><br><span class="line">        y_true = tf.equal(tf.cast(y_pred, tf.int32), tf.cast(y_true, tf.int32))</span><br><span class="line"></span><br><span class="line">        y_true = tf.cast(y_true, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            sample_weight = tf.cast(sample_weight, tf.float32)</span><br><span class="line">            y_true = tf.multiply(sample_weight, y_true)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.true_postives.assign_add(tf.reduce_sum(y_true))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.identity(self.true_postives)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_states</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.true_postives.assign(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=keras.optimizers.RMSprop(<span class="number">1e-3</span>),</span><br><span class="line">             loss=keras.losses.SparseCategoricalCrossentropy(),</span><br><span class="line">             metrics=[CatgoricalTruePostives()])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train,batch_size=<span class="number">64</span>, epochs=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以定义网络层的方式添加网络loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActivityRegularizationLayer</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        self.add_loss(tf.reduce_sum(inputs) * <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">784</span>,), name=<span class="string">&#x27;mnist_input&#x27;</span>)</span><br><span class="line">h1 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(inputs)</span><br><span class="line">h1 = ActivityRegularizationLayer()(h1)</span><br><span class="line">h1 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">outputs = layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(h1)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line"><span class="comment"># keras.utils.plot_model(model, &#x27;net001.png&#x27;, show_shapes=True)</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=keras.optimizers.RMSprop(),</span><br><span class="line">             loss=keras.losses.SparseCategoricalCrossentropy(),</span><br><span class="line">             metrics=[keras.metrics.SparseCategoricalAccuracy()])</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络层的方式添加要统计的metric</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MetricLoggingLayer</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        self.add_metric(keras.backend.std(inputs),</span><br><span class="line">                       name=<span class="string">&#x27;std_of_activation&#x27;</span>,</span><br><span class="line">                       aggregation=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">784</span>,), name=<span class="string">&#x27;mnist_input&#x27;</span>)</span><br><span class="line">h1 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(inputs)</span><br><span class="line">h1 = MetricLoggingLayer()(h1)</span><br><span class="line">h1 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">outputs = layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(h1)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line"><span class="comment"># keras.utils.plot_model(model, &#x27;net001.png&#x27;, show_shapes=True)</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=keras.optimizers.RMSprop(),</span><br><span class="line">             loss=keras.losses.SparseCategoricalCrossentropy(),</span><br><span class="line">             metrics=[keras.metrics.SparseCategoricalAccuracy()])</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 直接在model上面加</span></span><br><span class="line"><span class="comment"># 以定义网络层的方式添加要统计的metric</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MetricLoggingLayer</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        self.add_metric(keras.backend.std(inputs),</span><br><span class="line">                       name=<span class="string">&#x27;std_of_activation&#x27;</span>,</span><br><span class="line">                       aggregation=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">inputs = keras.Input(shape=(<span class="number">784</span>,), name=<span class="string">&#x27;mnist_input&#x27;</span>)</span><br><span class="line">h1 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(inputs)</span><br><span class="line">h2 = layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(h1)</span><br><span class="line">outputs = layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(h2)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line"></span><br><span class="line">model.add_metric(keras.backend.std(inputs),</span><br><span class="line">                       name=<span class="string">&#x27;std_of_activation&#x27;</span>,</span><br><span class="line">                       aggregation=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">model.add_loss(tf.reduce_sum(h1)*<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># keras.utils.plot_model(model, &#x27;net001.png&#x27;, show_shapes=True)</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=keras.optimizers.RMSprop(),</span><br><span class="line">             loss=keras.losses.SparseCategoricalCrossentropy(),</span><br><span class="line">             metrics=[keras.metrics.SparseCategoricalAccuracy()])</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">32</span>, epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>



<p>人工搭建神经网络</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(10): <span class="comment"># 设置10个epochs</span></span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 每一个batch批次</span></span><br><span class="line">        <span class="comment"># x:[128, 28, 28]</span></span><br><span class="line">        <span class="comment"># y:[128]</span></span><br><span class="line">        x = tf.reshape(x, [-1, 28*28])<span class="comment"># [batch, 28, 28] =&gt; [batch, 28×28]</span></span><br><span class="line">        with tf.GradientTape() as tape: <span class="comment"># 梯度带GradientTape</span></span><br><span class="line">            <span class="comment"># x: [batch, 28*28]</span></span><br><span class="line">            <span class="comment"># w1: [784,256]</span></span><br><span class="line">            <span class="comment"># b: [256]</span></span><br><span class="line">            <span class="comment"># h1 = x@w1 + b1</span></span><br><span class="line">            <span class="comment"># h1 = [b,784]@[784,256]+[256]</span></span><br><span class="line">            <span class="comment"># h1 = [b,256]+[256]</span></span><br><span class="line">            <span class="comment"># h1 = [b,256]+[b,256]</span></span><br><span class="line">            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])</span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            <span class="comment"># [b, 256] =&gt; [b, 128]</span></span><br><span class="line">            h2 = tf.add(h1@w2,b2)</span><br><span class="line">            h2 = tf.nn.relu(h2)</span><br><span class="line">            <span class="comment"># [b, 128] =&gt; [b, 10]</span></span><br><span class="line">            out = h2@w3 + b3</span><br><span class="line">            <span class="comment"># 输出向量: [b, 10]</span></span><br><span class="line">            </span><br><span class="line">            y_onehot = tf.one_hot(y, depth=10)<span class="comment"># y:[b] =&gt; [b, 10]</span></span><br><span class="line"></span><br><span class="line">            loss = tf.square(y_onehot - out) <span class="comment"># mse = mean(sum(y-out)^2)</span></span><br><span class="line">            </span><br><span class="line">            loss = tf.reduce_mean(loss)<span class="comment"># 计算平均误差</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])</span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad</span></span><br><span class="line">        w1.assign_sub(lr * grads[0])<span class="comment"># 原地更新，保持w1为Variable类型</span></span><br><span class="line">        b1.assign_sub(lr * grads[1])</span><br><span class="line">        w2.assign_sub(lr * grads[2])</span><br><span class="line">        b2.assign_sub(lr * grads[3])</span><br><span class="line">        w3.assign_sub(lr * grads[4])</span><br><span class="line">        b3.assign_sub(lr * grads[5])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % 100 == 0: <span class="comment"># 每一百步输出一次loss </span></span><br><span class="line">            <span class="built_in">print</span>(epoch, step, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/09/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94TensorFlow2.0/" data-id="ckmzn8px6001hj2tddigz3kqg" data-title="深度学习工具实践——TensorFlow2.0" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/09/17/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Vue/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          日常笔记——Vue
        
      </div>
    </a>
  
  
    <a href="/2020/09/17/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Session&Cookie&Token/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">日常笔记——Session&amp;Cookie&amp;Token</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Django/">Django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SQL/">SQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Study/">Study</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Testing/">Testing</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cos/">cos</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/idea/">idea</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/04/02/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94urljoin/">日常笔记——urljoin</a>
          </li>
        
          <li>
            <a href="/2021/04/01/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94k8s/">日常笔记——k8s</a>
          </li>
        
          <li>
            <a href="/2021/04/01/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2021/04/01/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Python/">Python function / package</a>
          </li>
        
          <li>
            <a href="/2021/03/31/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Elasticsearch/">日常笔记——Elasticsearch</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Jimmy Guo<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>